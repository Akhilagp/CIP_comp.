{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import array\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "#import dataset\n",
    "\n",
    "class FeatureExtractor(object):\n",
    "\t\"\"\"docstring for ClassName\"\"\"\n",
    "\tdef __init__(self, domain_list):\n",
    "\t\tself._domain_list = domain_list\n",
    "\t\tself._positive_domain_list = None\n",
    "\t\tself._hmm_model_path = './models/hmm.pkl'\n",
    "\t\tself._big_grame_model_path = './models/big_grame.pkl'\n",
    "\t\tself._triple_grame_model_path = './models/tripple_gram.pkl'\n",
    "\t\tself._positive_grame_model_path = './models/positive_grame.pkl'\n",
    "\t\tself._word_grame_model_path = './models/word_grame.pkl'\n",
    "\t\tself._positive_count_matrix = './models/positive_count_matrix.npy'\n",
    "\t\tself._word_count_matrix = './models/word_count_matrix.npy' \n",
    "\t\tself._positive_domain_list = self._load_positive_domain()\n",
    "\t\t#print(self._domain_list[:10])\n",
    "\t# check wether required files exis \n",
    "\tdef _check_files(self, *args):\n",
    "\t\tfor val in args:\n",
    "\t\t\tif not os.path.exists(val):\n",
    "\t\t\t\traise ValueError(\"file{} doesn't exis, check scripts \\\n",
    "\t\t\t\t\tdataset and prepare_model \".format(val))\n",
    "\n",
    "\n",
    "\tdef _load_positive_domain(self):\n",
    "\t\tpositive = pd.read_csv('../datas/aleax100k.csv', names=['domain'], header=None, dtype={'word': np.str}, encoding='utf-8')\n",
    "\t\tpositive = positive.dropna()\n",
    "\t\tpositive = positive.drop_duplicates()\n",
    "\t\treturn positive['domain'].tolist()\n",
    "\n",
    "\tdef count_aeiou(self):\n",
    "\t\tcount_result = []\n",
    "\t\tfor domain in self._domain_list:\n",
    "\t\t\tlen_aeiou = len(re.findall(r'[aeiou]',domain.lower()))\n",
    "\t\t\taeiou_rate = (0.0+len_aeiou)/len(domain)\n",
    "\t\t\ttmp = [domain, len(domain), len_aeiou, aeiou_rate]\n",
    "\t\t\tcount_result.append(tmp)\n",
    "\n",
    "\t\tcount_result = pd.DataFrame(count_result, \n",
    "\t\t\t\t\t\t\t\t\tcolumns=['domain','domain_len',\n",
    "\t\t\t\t\t\t\t\t\t\t\t 'aeiou_len','aeiou_rate'])\n",
    "\t\treturn count_result\n",
    "\n",
    "\n",
    "\t#the rate between original domain length and seted domain length\n",
    "\t\n",
    "\tdef unique_char_rate(self):\n",
    "\t\tunique_rate_list = []\n",
    "\t\tfor domain in self._domain_list:\n",
    "\t\t\tunique_len = len(set(domain))\n",
    "\t\t\tunique_rate = (unique_len+0.0)/len(domain)\n",
    "\t\t\ttmp = [domain, unique_len, unique_rate]\n",
    "\t\t\tunique_rate_list.append(tmp)\n",
    "\n",
    "\t\tunique_rate_df = pd.DataFrame(unique_rate_list, \n",
    "\t\t\t\t\t\t\t\t\t\tcolumns=['domain','unique_len','unique_rate'])\n",
    "\t\treturn unique_rate_df\n",
    "\n",
    "\n",
    "\t# calculate double domain's jarccard index\n",
    "\tdef _jarccard2domain(self, domain_aplha, domain_beta):\n",
    "\t\t\"\"\"parameters:\n",
    "\t\tdomain_alpha/beta: string-like domain\n",
    "\t\treturns: this couples jarccard index\n",
    "\t\t\"\"\"\n",
    "\t\tlistit_domain_alpha = list(domain_aplha)\n",
    "\t\tlistit_domain_beta = list(domain_beta)\n",
    "\n",
    "\t\tabs_intersection = np.intersect1d(listit_domain_alpha, listit_domain_beta).shape[0]\n",
    "\t\tabs_union = np.union1d(listit_domain_alpha, listit_domain_beta).shape[0]\n",
    "\t\t\n",
    "\t\treturn abs_intersection/abs_union*1.0\n",
    "\n",
    "\n",
    "\t# calculate each fake domain's average corresponding jarccard index \n",
    "\t# with positive domain collection\n",
    "\tdef jarccard_index(self):\n",
    "\t\t\"\"\"parameters:\n",
    "\t\tpositive_domain_list: positve samples list, 1Darray like\n",
    "\t\treturn: a pandas DataFrame, \n",
    "\t\t\t\tcontains domian col and average jarccard index col\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tpositive_domain_list = np.random.choice(self._positive_domain_list,500)\n",
    "\t\t#print(\"pdt: \",positive_domain_list)\n",
    "\t\tpositive_domain_list = positive_domain_list.tolist()\n",
    "\t\t#print(\"pdt: \",positive_domain_list)\n",
    "\n",
    "\t\tjarccard_index_list = []\n",
    "\t\tfor fake_domain in self._domain_list:\n",
    "\t\t\ttotal_jarccard_index = 0.0\n",
    "\t\t\tfor std_domain in positive_domain_list:\n",
    "\t\t\t\ttotal_jarccard_index += self._jarccard2domain(fake_domain, std_domain)\n",
    "\t\t\t\n",
    "\t\t\tavg_jarccard_index = total_jarccard_index/len(positive_domain_list)\n",
    "\t\t\ttmp = [fake_domain, avg_jarccard_index]\n",
    "\t\t\tjarccard_index_list.append(tmp)\n",
    "\n",
    "\t\tjarccard_index_df = pd.DataFrame(jarccard_index_list, \n",
    "\t\t\t\t\t\t\t\t\t\t\tcolumns=['domain','avg_jarccard_index'])\n",
    "\n",
    "\t\treturn jarccard_index_df\n",
    "\t\n",
    "\t'''\n",
    "\tdef levenshtein_distance(self,domain_alpha,domain_beta,mx=-1):\n",
    "\t\tdef result(d): \n",
    "\t\t\treturn d if mx < 0 else False if d > mx else True\n",
    "\t\tif domain_alpha == domain_beta:\n",
    "\t\t\treturn result(0)\n",
    "\t\tla, lb = len(domain_alpha), len(domain_beta)\n",
    "\t\tif mx >= 0 and abs(la - lb) > mx:\n",
    "\t\t\treturn result(mx+1)\n",
    "\t\tif la == 0:\n",
    "\t\t\treturn result(lb)\n",
    "\t\tif lb == 0:\n",
    "\t\t\treturn result(la)\n",
    "\t\tif lb > la:\n",
    "\t\t\tdomain_alpha, domain_beta, la, lb = domain_beta, domain_alpha, lb, la\n",
    " \n",
    "\t\tcost = array.array('i', range(lb + 1))\n",
    "\t\tfor i in range(1, la + 1):\n",
    "\t\t\tcost[0] = i\n",
    "\t\t\tls = i-1\n",
    "\t\t\tmn = ls\n",
    "\t\t\tfor j in range(1, lb + 1):\n",
    "\t\t\t\tls, act = cost[j], ls + int(domain_alpha[i-1] != domain_beta[j-1])\n",
    "\t\t\t\tcost[j] = min(ls+1, cost[j-1]+1, act)\n",
    "\t\t\t\tif (ls < mn):\n",
    "\t\t\t\t\tmn = ls\n",
    "\t\t\tif mx >= 0 and mn > mx:\n",
    "\t\t\t\treturn result(mx+1)\n",
    "\t\tif mx >= 0 and cost[lb] > mx:\n",
    "\t\t\treturn result(mx+1)\n",
    "\t\treturn result(cost[lb])\n",
    "\n",
    "\tdef levenshtein_distance_param(self):\n",
    "\t\tlevenshtein_distance_list=[]\n",
    "\t\tfor domain_index in range(1,len(self._domain_list)):\n",
    "\t\t\tlevenshtein_val = self.levenshtein_distance(self._domain_list[domain_index-1],self._domain_list[domain_index])\n",
    "\t\t\ttmp = [self._domain_list[domain_index],levenshtein_val]\n",
    "\t\t\tlevenshtein_distance_list.append(tmp)\n",
    "\t\tlevenshtein_distance_df = pd.DataFrame(levenshtein_distance_list,columns=['domain','levenshtein_distance'])\n",
    "\t\t\n",
    "\t\treturn levenshtein_distance_df\n",
    "\t'''\n",
    "\n",
    "\tdef findValidWords(self):\n",
    "\t\tmeaningful_word_ratio_list = []\n",
    "\t\tenglish_vocab = set(line.strip() for line in open('wordlist.txt','r'))\n",
    "\t\tfor st in self._domain_list:\n",
    "\t\t\ttot=0.0\n",
    "\t\t\tall_words = {st[i:j + i] for j in range(2, len(st)) for i in range(len(st)- j + 1)}\n",
    "\t\t\tint_words = all_words.intersection(english_vocab)\n",
    "\t\t\tfor i in int_words:\n",
    "\t\t\t\tif not any([i in sub_str for sub_str in int_words if i != sub_str]):\n",
    "\t\t\t\t\ttot+=len(i)\n",
    "\t\t\tmeaningful_word_ratio = tot/len(st)\n",
    "\t\t\ttmp = [st, meaningful_word_ratio]\n",
    "\t\t\tmeaningful_word_ratio_list.append(tmp)\n",
    "\t\t\n",
    "\t\tmeaningful_word_ratio_df = pd.DataFrame(meaningful_word_ratio_list, columns=['domain','meaningful_word_ratio'])\n",
    "\t\treturn meaningful_word_ratio_df\n",
    "\t\n",
    "\n",
    "\n",
    "\tdef _domain2vec(domain):\n",
    "\t\tver = []\n",
    "\t\tfor i in range(0, len(domain)):\n",
    "\t\t\tver.append([ord(domain[i])])\n",
    "\t\treturn ver\n",
    "\tdef positive_train(big_domain):\n",
    "        \t\"\"\"parameters:\n",
    "        \tbig domain: large scale posotive domains, type:list\n",
    "        \t\"\"\"\n",
    "        \tvec = CountVectorizer(analyzer='char', ngram_range=(3,5), min_df=1e-5, max_df=1.0)\n",
    "        \tgrame_model = vec.fit(big_domain)\n",
    "        \tjoblib.dump(grame_model, './models/positive_grame.pkl')\n",
    "        \tcounts_matrix = grame_model.transform(big_domain)\n",
    "        \tpositive_counts = np.log10(counts_matrix.sum(axis=0).getA1())\n",
    "        \tnp.save('./models/positive_count_matrix.npy' , positive_counts)\n",
    "\n",
    "\tdef word_train(word):\n",
    "        \tvec = CountVectorizer(analyzer='char', ngram_range=(3,5), min_df=1e-5, max_df=1.0)\n",
    "        \tgrame_model = vec.fit(word)\n",
    "        \tjoblib.dump(grame_model, './models/word_grame.pkl')\n",
    "        \tcounts_matrix = grame_model.transform(word)\n",
    "        \tword_counts =  np.log10(counts_matrix.sum(axis=0).getA1())\n",
    "        \tnp.save('./models/word_count_matrix.npy', word_counts)\n",
    "\n",
    "\n",
    "\t# TODO\n",
    "\tdef hmm_index(self):\n",
    "\t\thmm_model = joblib.load(self._hmm_model_path)\n",
    "\t\thmm_index_list = []\n",
    "\t\tfor domain in self._domain_list:\n",
    "\t\t\tvec = self._domain2vec(domian)\n",
    "\t\t\thmm_score = hmm_model.predict(vec)\n",
    "\t\t\ttmp = [domain, hmm_score]\n",
    "\t\t\thmm_index_list.append(tmp)\n",
    "\n",
    "\t\thmm_index_list = pd.DataFrame(hmm_index_list, columns=['domain','hmm_index'])\n",
    "\n",
    "\t\treturn hmm_index_list\n",
    "\n",
    "\n",
    "\t'''\n",
    "\t# calculate n_grame of each domains\n",
    "\t# notes: you should update this model frequency\n",
    "\t# \t\tdecrease the dimension of the transformed data set\n",
    "\t# \t\tand rank features\n",
    "\tdef big_grame(self):\n",
    "\t\tif not os.path.exists(self._n_grame_model_path):\n",
    "\t\t\traise(\"n_grame model dosen't exists, try to training this model\\n\\\n",
    "\t\t\t\ttrain scripts at same level folder called prepare_model.py\\n\\\n",
    "\t\t\t\tnotes: training n_grame model by domains as much as you have\")\n",
    "\n",
    "\t\tgrame_model = joblib.load(self._n_grame_model_path)\n",
    "\t\tvec = grame_model.transform(np.array(self._domain_list))\n",
    "\n",
    "\t\tdf = pd.DataFrame(vec.toarray(), columns=grame_model.get_feature_names())\n",
    "\t\tdomains = pd.DataFrame(self._domain_list, columns=['domain'])\n",
    "\t\tdf = pd.concat([domains, df], axis=1)\n",
    "\t\t\n",
    "\t\treturn df\n",
    "\n",
    "\tdef tripple_gram(self):\n",
    "\t\tpass\n",
    "\t\n",
    "\t'''\n",
    "\n",
    "\n",
    "\t#calculate entropy of domains entropy\n",
    "\tdef entropy(self):\n",
    "\t\t\"\"\"parameters\n",
    "\n",
    "\t\treturn: entropy DataFrame [doamin, entropy]\n",
    "\t\t\"\"\"\n",
    "\t\tentropy_list = []\n",
    "\t\tfor domain in self._domain_list:\n",
    "\t\t\tp, lns = Counter(domain), float(len(domain))\n",
    "\t\t\tentropy = (-sum(count/lns * math.log(count/lns, 2) for count in p.values()))\n",
    "\t\t\ttmp = [domain, entropy]\n",
    "\t\t\tentropy_list.append(tmp)\n",
    "\n",
    "\t\tentropy_df = pd.DataFrame(entropy_list, columns=['domain','entropy'])\n",
    "\t\treturn entropy_df\n",
    "\n",
    "\n",
    "\n",
    "\t#calculate grame(3,4,5) and its differ\n",
    "\tdef n_grame(self):\n",
    "\t\t\"\"\"\n",
    "\t\treturn local grame differ with positive domains and word list\n",
    "\t\t\"\"\"\n",
    "\t\tself._check_files(self._positive_count_matrix,\n",
    "\t\t\t\t\t\t  self._positive_grame_model_path,\n",
    "\t\t\t\t\t\t  self._word_grame_model_path,\n",
    "\t\t\t\t\t\t  self._word_count_matrix)\n",
    "\t\t\n",
    "\t\tpositive_count_matrix = np.load(self._positive_count_matrix)\n",
    "\t\tpositive_vectorizer = joblib.load(self._positive_grame_model_path)\n",
    "\t\tword_count_matrix = np.load(self._word_count_matrix)\n",
    "\t\tword_vectorizer = joblib.load(self._word_grame_model_path)\n",
    "\n",
    "\t\tpositive_grames = positive_count_matrix * positive_vectorizer.transform(self._domain_list).T\n",
    "\t\tword_grames = word_count_matrix * word_vectorizer.transform(self._domain_list).T\n",
    "\t\tdiff = positive_grames - word_grames\n",
    "\t\tdomains = np.asarray(self._domain_list)\n",
    "\n",
    "\n",
    "\t\tn_grame_nd = np.c_[domains, positive_grames, word_grames, diff]\n",
    "\t\tn_grame_df = pd.DataFrame(n_grame_nd, columns=['domain','positive_grames','word_grames','diff'])\n",
    "\t\t\n",
    "\t\treturn n_grame_df\n",
    "\n",
    "\tdef digit_ratio(self):\n",
    "\t\tdigit_pattern = r'\\d'\n",
    "\t\tcharacter_pattern = r'[A-Za-z]'\n",
    "\t\tcharacter_finder = re.compile(character_pattern)\n",
    "\t\tdigit_finder = re.compile(digit_pattern)\n",
    "\t\tdigit_ration_list = []\n",
    "\n",
    "\t\tfor domain in self._domain_list:\n",
    "\t\t\tdigit_len = len(digit_finder.findall(domain))\n",
    "\t\t\tcharacter_len = len(character_finder.findall(domain))\n",
    "\t\t\tdomain_len = len(domain)\n",
    "\t\t\tdigit_ratio = (digit_len+0.0)/domain_len\n",
    "\t\t\tcharacter_ratio = (character_len+0.0)/domain_len\n",
    "\t\t\ttmp = [domain, digit_ratio, character_ratio]\n",
    "\t\t\tdigit_ration_list.append(tmp)\n",
    "\t\t\n",
    "\t\tdigit_ration_df = pd.DataFrame(digit_ration_list, columns=['domain',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   'digit_ration',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   'character_ratio']\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   )\n",
    "\n",
    "\t\treturn digit_ration_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_feature(domain_list):\n",
    "\textractor = FeatureExtractor(domain_list)\n",
    "\n",
    "\tprint(\"Extracting count_aeiou....\")\n",
    "\taeiou_df = extractor.count_aeiou()\n",
    "\tprint(\"Extracted count_aeiou, shape is %d\\n\" % aeiou_df.shape[0])\n",
    "\n",
    "\tprint(\"Extracting unique_rate....\")\n",
    "\tunique_rate_df = extractor.unique_char_rate()\n",
    "\tprint(\"Extracted unique_rate, shape is %d\\n\" % unique_rate_df.shape[0])\n",
    "\n",
    "\tprint(\"Extracting jarccard_index....\")\n",
    "\tjarccard_index_df = extractor.jarccard_index()\n",
    "\tprint(\"Extracted jarccard_index.....\\n\")\n",
    "\n",
    "\tprint(\"Extracting entropy....\")\n",
    "\tentropy_df = extractor.entropy()\n",
    "\tprint(\"Extracted entropy, shape is %d\\n\"%entropy_df.shape[0])\n",
    "\t\n",
    "\tprint(\"Extracting n_grame....\")\n",
    "\tn_grame_df = extractor.n_grame()\n",
    "\tprint(\"Extracted n_grame, shape is %d\\n\"%n_grame_df.shape[0])\n",
    "\t#levenshtein_distance_df = extractor.levenshtein_distance_param()\n",
    "\tmeaningful_word_ratio_df= extractor.findValidWords()\n",
    "\tprint(\"Merge all features on domains...\")\n",
    "\tmultiple_df = [aeiou_df, unique_rate_df, \n",
    "\t\t\t\t  entropy_df, jarccard_index_df,\n",
    "\t\t\t\t  n_grame_df,meaningful_word_ratio_df]\n",
    "\n",
    "\tdf_final = reduce(lambda left,right: pd.merge(left,right,on='domain',how='left'), multiple_df)\n",
    "\t#print(\"df_final\",df_final.dtypes)\n",
    "\tprint(\"Merged all features, shape is %d\\n\"%df_final.shape[0])\n",
    "\t# check df\n",
    "\tstd_rows = aeiou_df.shape[0]\n",
    "\tdf_final_rows = df_final.shape[0]\n",
    "\n",
    "\tif std_rows != df_final_rows:\n",
    "\t\traise(\"row dosen't match after Merged multiple_df\")\n",
    "\n",
    "\tdf_final = df_final.drop(['domain'],axis=1)\n",
    "\tdf_final = df_final.round(3)\n",
    "\t\n",
    "\treturn np.array(df_final),df_final.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#In [ ]:\n",
    "import sys\n",
    "from sklearn import feature_extraction\n",
    "\n",
    "#In [ ]:\n",
    "\n",
    "import dataset\n",
    "#from feature import  get_feature\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#In [ ]:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_simple_data():\n",
    "\tfiles = os.listdir('./dgadetec/AlgorithmPowereddomains')\n",
    "\t\n",
    "\tdomain_list = []\n",
    "\tfor f in files:\n",
    "\t\tpath = './dgadetec/AlgorithmPowereddomains/'+f\n",
    "\t\tdomains = pd.read_csv(path,names=['domain'])\n",
    "\t\tdomains = domains['domain'].tolist()\n",
    "\t\tfor item in domains:\n",
    "\t\t\tdomain_list.append(item)\n",
    "\treturn domain_list\n",
    "\n",
    "def load_data():\n",
    "\tif os.path.exists('../datas/train.npy'):\n",
    "\t\ttrain = np.load('../datas/train.npy')\n",
    "\t\treturn train\n",
    "\ttrain = pd.read_csv(\"blah.csv\",header=None,names=['domain','label'])\n",
    "\tprint(train.head(5))\n",
    "\tprint(\"tr\",train.dtypes)\n",
    "\ttrain1 = np.array(train.values)\n",
    "\tprint(np.where(np.isnan(train1)))\n",
    "\tnp.random.shuffle(train1)\n",
    "\tnp.save('../datas/train.npy', train1)\n",
    "\n",
    "\treturn train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159991</th>\n",
       "      <td>nhxoriyt2p7kukrwf4.net</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159992</th>\n",
       "      <td>1j0iuoynsiyowtg0sz711luz7j.net</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159993</th>\n",
       "      <td>dvgkjesfwtq.net</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159994</th>\n",
       "      <td>dizipwectyw.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159995</th>\n",
       "      <td>cohcqqeoya.info</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159996</th>\n",
       "      <td>ynjblsdsholapet.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159997</th>\n",
       "      <td>djhuiycbjicufmdmx.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159998</th>\n",
       "      <td>myzkbnlin3tngdg.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159999</th>\n",
       "      <td>ywkmenhancedysb.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                domain label\n",
       "159991          nhxoriyt2p7kukrwf4.net     1\n",
       "159992  1j0iuoynsiyowtg0sz711luz7j.net     1\n",
       "159993                 dvgkjesfwtq.net     1\n",
       "159994                 dizipwectyw.com     1\n",
       "159995                 cohcqqeoya.info     1\n",
       "159996             ynjblsdsholapet.com     1\n",
       "159997           djhuiycbjicufmdmx.com     1\n",
       "159998             myzkbnlin3tngdg.com     1\n",
       "159999             ywkmenhancedysb.com     1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data()\n",
    "data = pd.DataFrame(data, columns=['domain', 'label'])\n",
    "data.tail(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160000, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset='domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158755, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = data[:10000,0]\n",
    "trainY = data[:10000,1].astype(int) \n",
    "testX = data[10001:15000, 0]\n",
    "testX_cluster = np.array(testX)\n",
    "testY = data[10001:15000, 1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting count_aeiou....\n",
      "Extracted count_aeiou, shape is 10000\n",
      "\n",
      "Extracting unique_rate....\n",
      "Extracted unique_rate, shape is 10000\n",
      "\n",
      "Extracting jarccard_index....\n",
      "Extracted jarccard_index.....\n",
      "\n",
      "Extracting entropy....\n",
      "Extracted entropy, shape is 10000\n",
      "\n",
      "Extracting n_grame....\n",
      "Extracted n_grame, shape is 10000\n",
      "\n",
      "Merge all features on domains...\n",
      "Merged all features, shape is 10000\n",
      "\n",
      "Extracting count_aeiou....\n",
      "Extracted count_aeiou, shape is 4999\n",
      "\n",
      "Extracting unique_rate....\n",
      "Extracted unique_rate, shape is 4999\n",
      "\n",
      "Extracting jarccard_index....\n",
      "Extracted jarccard_index.....\n",
      "\n",
      "Extracting entropy....\n",
      "Extracted entropy, shape is 4999\n",
      "\n",
      "Extracting n_grame....\n",
      "Extracted n_grame, shape is 4999\n",
      "\n",
      "Merge all features on domains...\n",
      "Merged all features, shape is 4999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainX,trainX_head = get_feature(trainX)\n",
    "testX,testX_head = get_feature(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain_len</th>\n",
       "      <th>aeiou_len</th>\n",
       "      <th>aeiou_rate</th>\n",
       "      <th>unique_len</th>\n",
       "      <th>unique_rate</th>\n",
       "      <th>entropy</th>\n",
       "      <th>avg_jarccard_index</th>\n",
       "      <th>positive_grames</th>\n",
       "      <th>word_grames</th>\n",
       "      <th>diff</th>\n",
       "      <th>meaningful_word_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0.158</td>\n",
       "      <td>14</td>\n",
       "      <td>0.737</td>\n",
       "      <td>3.722</td>\n",
       "      <td>0.372</td>\n",
       "      <td>32.5359</td>\n",
       "      <td>14.2761</td>\n",
       "      <td>18.2598</td>\n",
       "      <td>0.842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0.133</td>\n",
       "      <td>20</td>\n",
       "      <td>0.667</td>\n",
       "      <td>4.215</td>\n",
       "      <td>0.296</td>\n",
       "      <td>39.1698</td>\n",
       "      <td>10.7722</td>\n",
       "      <td>28.3976</td>\n",
       "      <td>0.567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.200</td>\n",
       "      <td>13</td>\n",
       "      <td>0.867</td>\n",
       "      <td>3.640</td>\n",
       "      <td>0.314</td>\n",
       "      <td>25.5836</td>\n",
       "      <td>3.56122</td>\n",
       "      <td>22.0224</td>\n",
       "      <td>0.267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>7</td>\n",
       "      <td>0.189</td>\n",
       "      <td>18</td>\n",
       "      <td>0.486</td>\n",
       "      <td>3.972</td>\n",
       "      <td>0.211</td>\n",
       "      <td>13.9136</td>\n",
       "      <td>5.63447</td>\n",
       "      <td>8.27912</td>\n",
       "      <td>0.243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>0.294</td>\n",
       "      <td>14</td>\n",
       "      <td>0.824</td>\n",
       "      <td>3.735</td>\n",
       "      <td>0.374</td>\n",
       "      <td>25.0913</td>\n",
       "      <td>9.97922</td>\n",
       "      <td>15.1121</td>\n",
       "      <td>0.706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>0.067</td>\n",
       "      <td>23</td>\n",
       "      <td>0.767</td>\n",
       "      <td>4.415</td>\n",
       "      <td>0.257</td>\n",
       "      <td>30.8593</td>\n",
       "      <td>7.64271</td>\n",
       "      <td>23.2166</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>0.231</td>\n",
       "      <td>15</td>\n",
       "      <td>0.577</td>\n",
       "      <td>3.767</td>\n",
       "      <td>0.306</td>\n",
       "      <td>37.1658</td>\n",
       "      <td>26.8244</td>\n",
       "      <td>10.3415</td>\n",
       "      <td>0.962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.095</td>\n",
       "      <td>15</td>\n",
       "      <td>0.714</td>\n",
       "      <td>3.785</td>\n",
       "      <td>0.348</td>\n",
       "      <td>24.4635</td>\n",
       "      <td>28.4324</td>\n",
       "      <td>-3.96891</td>\n",
       "      <td>1.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>0.500</td>\n",
       "      <td>11</td>\n",
       "      <td>0.786</td>\n",
       "      <td>3.379</td>\n",
       "      <td>0.438</td>\n",
       "      <td>75.4451</td>\n",
       "      <td>45.8609</td>\n",
       "      <td>29.5842</td>\n",
       "      <td>1.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>0.130</td>\n",
       "      <td>17</td>\n",
       "      <td>0.739</td>\n",
       "      <td>4.002</td>\n",
       "      <td>0.336</td>\n",
       "      <td>33.8801</td>\n",
       "      <td>8.52372</td>\n",
       "      <td>25.3564</td>\n",
       "      <td>0.783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   domain_len  aeiou_len  aeiou_rate  unique_len  unique_rate  entropy  \\\n",
       "0          19          3       0.158          14        0.737    3.722   \n",
       "1          30          4       0.133          20        0.667    4.215   \n",
       "2          15          3       0.200          13        0.867    3.640   \n",
       "3          37          7       0.189          18        0.486    3.972   \n",
       "4          17          5       0.294          14        0.824    3.735   \n",
       "5          30          2       0.067          23        0.767    4.415   \n",
       "6          26          6       0.231          15        0.577    3.767   \n",
       "7          21          2       0.095          15        0.714    3.785   \n",
       "8          14          7       0.500          11        0.786    3.379   \n",
       "9          23          3       0.130          17        0.739    4.002   \n",
       "\n",
       "   avg_jarccard_index positive_grames word_grames     diff  \\\n",
       "0               0.372         32.5359     14.2761  18.2598   \n",
       "1               0.296         39.1698     10.7722  28.3976   \n",
       "2               0.314         25.5836     3.56122  22.0224   \n",
       "3               0.211         13.9136     5.63447  8.27912   \n",
       "4               0.374         25.0913     9.97922  15.1121   \n",
       "5               0.257         30.8593     7.64271  23.2166   \n",
       "6               0.306         37.1658     26.8244  10.3415   \n",
       "7               0.348         24.4635     28.4324 -3.96891   \n",
       "8               0.438         75.4451     45.8609  29.5842   \n",
       "9               0.336         33.8801     8.52372  25.3564   \n",
       "\n",
       "   meaningful_word_ratio  \n",
       "0                  0.842  \n",
       "1                  0.567  \n",
       "2                  0.267  \n",
       "3                  0.243  \n",
       "4                  0.706  \n",
       "5                  0.333  \n",
       "6                  0.962  \n",
       "7                  1.048  \n",
       "8                  1.071  \n",
       "9                  0.783  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simpleLR acc=0.9697939587917583 f1=0.9843084277252416\n",
      "simpleSVM Accuracy:0.9597919583916783 f1_score: 0.9597919583916783\n",
      "simpleGBM acc=0.9793958791758351 f1=0.9892674794206523\n",
      "simpleRF acc=0.9543908781756352 f1=0.9766584766584767\n",
      "simpleJ48 acc=0.9673934786957391 f1=0.9829086714899864\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "#In [23]:\n",
    "\n",
    "def metric_me(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 =f1_score(y_true, y_pred)\n",
    "    \n",
    "    return accuracy, f1\n",
    "\n",
    "#In [24]:\n",
    "simpleLR = LogisticRegression()\n",
    "simpleLR.fit(trainX, trainY)\n",
    "pred_y = simpleLR.predict(testX)\n",
    "acc, f1 = metric_me(testY, pred_y)\n",
    "print(\"simpleLR acc={} f1={}\".format(acc, f1))\n",
    "######################################################################\n",
    "\n",
    "simpleSVM = SVC()\n",
    "simpleSVM.fit(trainX,trainY)\n",
    "pred_y = simpleSVM.predict(testX)\n",
    "acc, f1 = metric_me(testY, pred_y)\n",
    "print(\"simpleSVM Accuracy:{0} f1_score: {0}\".format(acc, f1))\n",
    "###########################################################################3\n",
    "simpleGBM = GradientBoostingClassifier()\n",
    "simpleGBM.fit(trainX, trainY)\n",
    "pred_y = simpleGBM.predict(testX)\n",
    "acc, f1= metric_me(testY, pred_y)\n",
    "print(\"simpleGBM acc={} f1={}\".format(acc, f1))\n",
    "\n",
    "###########################################################################3\n",
    "simpleRF = RandomForestClassifier(n_estimators = 1000, max_depth = 2, random_state = 42)\n",
    "simpleRF.fit(trainX,trainY)\n",
    "pred_y = simpleRF.predict(testX)\n",
    "acc, f1 = metric_me(testY,pred_y.round())\n",
    "print(\"simpleRF acc={} f1={}\".format(acc, f1))\n",
    "###########################################################################3\n",
    "simpleJ48 = tree.DecisionTreeClassifier()\n",
    "simpleJ48.fit(trainX, trainY)\n",
    "pred_y = simpleJ48.predict(testX)\n",
    "acc, f1= metric_me(testY, pred_y)\n",
    "print(\"simpleJ48 acc={} f1={}\".format(acc, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 11)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 3, 0.214, 11, 0.786, 3.325, 0.285, 24.27579826895692,\n",
       "       17.237305813656675, 7.0384924553002435, 0.857], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4999,) (4999, 11) (4999,)\n"
     ]
    }
   ],
   "source": [
    "print(testX_cluster.shape,testX.shape,pred_y.shape)\n",
    "cluster_inp = np.concatenate((testX,testX_cluster[:,None]),axis=1)\n",
    "cluster_inp = np.concatenate((cluster_inp,pred_y[:,None]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4999, 13)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4999, 13)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar = cluster_inp\n",
    "ar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_1 = ar[ar[:,-1]==1]\n",
    "#print(ar_1)\n",
    "ar_1 = ar_1[:,:-2]\n",
    "#print(ar_1)\n",
    "inp_type =ar_1[:,-2:]\n",
    "#print(inp_type,type(inp_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4767, 11)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar_1.shape\n",
    "inp_type.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4767, 1)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:,None].shape\n",
    "#inp_type.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0  1  2  3  4]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-4c8cc60feb31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbscan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_2d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_2d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minp_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "dbscan = DBSCAN(eps=4.5, metric='euclidean', min_samples=13).fit(ar_1)\n",
    "pca = PCA(n_components=3).fit(ar_1)\n",
    "pca_2d = pca.transform(ar_1)\n",
    "print(np.unique(dbscan.labels_))\n",
    "y_pred = dbscan.fit_predict(pca_2d)\n",
    "op = np.concatenate((pca_2d,y_pred[:,None]),axis=1)\n",
    "tt = np.concatenate((y_pred[:,None],inp_type),axis=1)\n",
    "np.save(sys.argv[1],tt)\n",
    "op = op[op[:,-1]!=-1]\n",
    "y_pred = op[:,-1]\n",
    "plt.scatter(op[:,0], op[:,1],300,c=y_pred, cmap='Paired',label=y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
